{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Run this to attack a trained model via TrainModel. \n",
    "Use the \"loadFullModel\" submethod to load in an already trained model (trained via TrainModel)\n",
    "The main attack function is \"runAttacks\" which runs attacks on trained models\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "from cleverhans.attacks import Noise, CarliniWagnerL2, MaxConfidence, FastGradientMethod, BasicIterativeMethod, DeepFool, MomentumIterativeMethod, ProjectedGradientDescent\n",
    "from Model_Implementations import Model_Softmax_Baseline, Model_Logistic_Baseline, Model_Logistic_Ensemble, Model_Tanh_Ensemble, Model_Tanh_Baseline\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras import backend\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNo (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 64)        1664      \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_18  ((None, 28, 28, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_19  ((None, 14, 14, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_20  ((None, 14, 14, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_21  ((None, 7, 7, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_22  ((None, 7, 7, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_23  ((None, 4, 4, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_9 (L ((None, 128), (None, 64,  0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_10 ( ((None, 64), (None, 32, 2 0         \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_11 ( ((None, 64), (None, 32, 2 0         \n",
      "_________________________________________________________________\n",
      "sb__layer_3 (SB_Layer)       (None, 10)                1620      \n",
      "=================================================================\n",
      "Total params: 327,380\n",
      "Trainable params: 327,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Time:  1.4530494809150696e-05\n",
      "\n",
      "\n",
      "\n",
      "Running tests on model:  softmax_baseline_MNIST\n",
      "Clean accuracy of model:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-f86a7e0e6ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mmodels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd = runAttacks(models_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mprobs_adv_pgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_adv_pgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_adv_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunAttacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for ploting probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-255-f86a7e0e6ed0>\u001b[0m in \u001b[0;36mrunAttacks\u001b[0;34m(models_list)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clean accuracy of model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mprobs_benign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenignAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-f86a7e0e6ed0>\u001b[0m in \u001b[0;36mbenignAccuracy\u001b[0;34m(model, X, Y)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mprobs_benign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_benign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0macc_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = 'models/'  #path with saved model parameters \n",
    "sess =  backend.get_session()\n",
    "backend.set_learning_phase(0) #need to do this to get CleverHans to work with batchnorm\n",
    "\n",
    "\n",
    "#Dataset-specific parameters - should be same as those used in TrainModel\n",
    "\n",
    "#CIFAR\n",
    "# DATA_DESC = 'CIFAR10'; (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "# epochs=None; weight_save_freq=None\n",
    "# num_classes=10  #how many classes (categories) are in this dataset?\n",
    "# Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "# num_filters_std = [32, 64, 128]; num_filters_ens=[32, 64, 128]; num_filters_ens_2=16; dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "# model_rep_baseline=2; model_rep_ens=2; DATA_AUGMENTATION_FLAG=1; BATCH_NORMALIZATION_FLAG=1\n",
    "# num_channels = 3; inp_shape = (32,32,3); lr=1e-4; batch_size=80;\n",
    "# noise_stddev = 0.032; blend_factor = .032\n",
    "\n",
    "#MNIST\n",
    "DATA_DESC = 'MNIST'; (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "num_channels = 1; inp_shape = (28,28,1); num_classes=10\n",
    "#MODEL-SPECIFIC PARAMETERS: MNIST\n",
    "#PARAMETERS RELATED TO SGD OPTIMIZATION\n",
    "epochs=None; weight_save_freq=None; batch_size=80; lr=3e-4; \n",
    "#MODEL DEFINTION PARAMETERS\n",
    "num_filters_std = [64, 64, 64]; num_filters_ens=[32, 32, 32]; num_filters_ens_2=4; \n",
    "dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "noise_stddev = 0.3; blend_factor=0.3; \n",
    "model_rep_baseline=1; model_rep_ens=2; \n",
    "DATA_AUGMENTATION_FLAG=0; BATCH_NORMALIZATION_FLAG=0\n",
    "\n",
    "#Attack parameters\n",
    "eps_val = 8/255.0; PGD_iters = 200; eps_iter=(2/3)*eps_val; \n",
    "eps_range = np.linspace(0, 0.33, 10)\n",
    "noise_eps=0.1\n",
    "\n",
    "\n",
    "# DATA PRE-PROCESSING\n",
    "X_train = (X_train/255).astype(np.float32);  X_test = (X_test/255).astype(np.float32)\n",
    "#reshape (add third (image) channel)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],num_channels); X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "X_valid = X_test[1000:2000]; Y_valid = Y_test[1000:2000]; #validation data, used to attack model\n",
    "X_train = X_train-0.5; X_test = X_test-0.5; X_valid = X_valid-0.5; #map to range (-0.5,0.5)\n",
    "data_dict = {'X_train':X_train, 'Y_train_cat':Y_train, 'X_test':X_test, 'Y_test_cat':Y_test}\n",
    "X_random = np.random.rand(X_valid.shape[0],X_valid.shape[1],X_valid.shape[2],X_valid.shape[3])-0.5; X_random = X_random.astype(np.float32)\n",
    "\n",
    "\n",
    "# def output_activation(x):\n",
    "#     return tf.nn.softmax(x) \n",
    "\n",
    "#Model definition of the model we want to attack; should be same as the definition used in TrainModel\n",
    "name = 'softmax_baseline'+'_'+DATA_DESC; num_chunks=1\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "def output_activation(x):\n",
    "    return tf.nn.softmax(x) \n",
    "base_model=None\n",
    "params_dict = {'weight_decay':weight_decay, 'num_filters_std':num_filters_std, 'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'model_rep':model_rep_baseline, 'base_model':base_model, 'num_chunks':num_chunks, 'output_activation':output_activation,  'batch_size':batch_size, 'epochs':epochs, 'lr':lr, 'dropout_rate':dropout_rate_std,  'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "m4 = Model_Softmax_Baseline(data_dict, params_dict)\n",
    "m4.loadFullModel() #load in the saved model, which should have already been trained first via TrainModel\n",
    "\n",
    "m4.legend = 'TEns16'; \n",
    "m4.X_valid = X_valid; m4.Y_valid = Y_valid; \n",
    "m4.X_test = X_test; m4.Y_test = Y_test; \n",
    "m4.X_random = X_random; \n",
    "m4.minval = -0.5; m4.maxval = 0.5\n",
    "\n",
    "\n",
    "\n",
    "def benignAccuracy(model, X, Y):\n",
    "    \n",
    "    acc_vec=[]; probs_benign_list=[]\n",
    "    for rep in np.arange(0, X.shape[0], 1000):\n",
    "        x = X[rep:rep+1000]\n",
    "        probs_benign = sess.run(model.predict(tf.convert_to_tensor(x))) \n",
    "        acc = np.mean(np.argmax(probs_benign, 1)==Y[rep:rep+1000])\n",
    "        acc_vec += [acc]\n",
    "        probs_benign_list += list(np.max(probs_benign, 1))\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_benign_list\n",
    "\n",
    "\n",
    "def wbAttack(model, attack, att_params, X, Y):\n",
    "    sess =  backend.get_session()\n",
    "    modelCH = model.modelCH()\n",
    "    adv_model = attack(modelCH, sess=sess) \n",
    "    \n",
    "    acc_vec=[]; probs_adv_list=[]\n",
    "    inc=250\n",
    "    for rep in np.arange(0, X.shape[0], inc):\n",
    "        x = X[rep:rep+inc]\n",
    "        y = Y[rep:rep+inc]\n",
    "        X_adv = adv_model.generate(tf.convert_to_tensor(x), **att_params).eval(session=sess)  \n",
    "        print(X_adv.dtype)\n",
    "        preds = np.argmax(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        acc =  np.mean(np.equal(preds, y))\n",
    "        probs_adv = np.max(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        probs_adv = probs_adv[preds != y]\n",
    "        acc= np.mean(np.equal(preds, y))\n",
    "        acc_vec += [acc]\n",
    "        probs_adv_list += list(probs_adv)\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Adv accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_adv_list, acc, X_adv, y\n",
    "\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "\n",
    "\n",
    "def runAttacks(models_list):\n",
    "    #CW attack\n",
    "    for model in models_list:\n",
    "        \n",
    "        print(\"\"); print(\"\"); print(\"\");\n",
    "        print(\"Running tests on model: \", model.params_dict['name'])\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Clean accuracy of model:\")\n",
    "        probs_benign = benignAccuracy(model, model.X_test, model.Y_test)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('Benign Time: ', stop - start)  \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running PGD attack:\")\n",
    "        att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':eps_val, 'eps_iter':eps_iter, 'nb_iter':PGD_iters,'ord':np.inf}\n",
    "        probs_adv_pgd, junk, X_adv_pgd, y_adv_pgd = wbAttack(model, ProjectedGradientDescent, att_params, model.X_valid, model.Y_valid)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('PGD Time: ', stop - start) \n",
    "        \n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running CW attack:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "#         probs_adv_cw, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, model.X_valid[0:100], model.Y_valid[0:100])\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('CW Time: ', stop - start) \n",
    "        \n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running Blind Spot attack, alpha=0.8:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "#         probs_adv_bsa, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, 0.8*model.X_valid[0:100], model.Y_valid[0:100])\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('BSA Time: ', stop - start) \n",
    "                \n",
    "        #Random ATTACK (0 SNR inputs)\n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running random attack:\")\n",
    "#         probs_random = np.max(sess.run(model.predict(tf.convert_to_tensor(model.X_random))), 1)\n",
    "#         print('Prob. that ', model.params_dict['name'], ' < 0.9 on random data: ', np.mean(probs_random<0.9))\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('Random Time: ', stop - start) \n",
    "        \n",
    "#         #Noise ATTACK (low SNR inputs)\n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running Noise attack:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':noise_eps}\n",
    "#         probs_noise, junk, X_adv, y = wbAttack(model, Noise, att_params, model.X_valid, model.Y_valid)\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('Running Noise Time: ', stop - start) \n",
    "        \n",
    "#     return probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd\n",
    "    return probs_adv_pgd, X_adv_pgd, y_adv_pgd #for plotting probs\n",
    "\n",
    "\n",
    "\n",
    "models_list = [m4]\n",
    "# probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd = runAttacks(models_list)\n",
    "probs_adv_pgd, X_adv_pgd, y_adv_pgd = runAttacks(models_list) #for ploting probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  1.1\n",
      "Iterating over 5 batches\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# DAA 4rd try according to tf2 cleverhans \n",
    "\n",
    "# WHY IS X_ADV NEGATIVE?\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "c = 1.1\n",
    "import time\n",
    "\n",
    "class LinfBLOBAttack:\n",
    "    def __init__(self, model, epsilon, k, a, random_start):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self.a = a\n",
    "        self.rand = random_start\n",
    "\n",
    "        c = 1.1\n",
    "        print('c: ', c)\n",
    "\n",
    "    def perturb(self, x_nat, x_adv, y, sess): \n",
    "        batch_size = x_adv.shape[0]\n",
    "        for epoch in range(10):\n",
    "            print(epoch)\n",
    "            x_adv = tf.convert_to_tensor(x_adv)\n",
    "            x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "            with tf.GradientTape(persistent=True) as g:\n",
    "                g.watch(x_adv)\n",
    "    #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "    #             out = tf.expand_dims(out, axis=1)\n",
    "    #             out = tf.dtypes.cast(out, tf.float64)\n",
    "    #             y = tf.dtypes.cast(y, tf.int64)\n",
    "    #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "                out = m4.model(x_adv)\n",
    "                y = tf.expand_dims(y, axis=1)\n",
    "                out = tf.dtypes.cast(out, tf.float32)\n",
    "                y = tf.dtypes.cast(y, tf.int64)\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "\n",
    "            grad = g.gradient(loss, x_adv)\n",
    "            grad = grad.eval(session=sess)\n",
    "            x_adv = x_adv.eval(session=sess)\n",
    "            grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "            kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "            x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "            x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "            #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "            x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "            x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "            x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "        return x_adv\n",
    "    \n",
    "    def svgd_kernel(self, theta):\n",
    "        theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "        h = np.median(pairwise_dists)  \n",
    "        h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import json\n",
    "    import sys\n",
    "    import math\n",
    "    \n",
    "    sess =  backend.get_session()\n",
    "\n",
    "    attack = LinfBLOBAttack(m4,\n",
    "                         epsilon=0.3,\n",
    "                         k=200,\n",
    "                         a=0.01,\n",
    "                         random_start=True)\n",
    "\n",
    "    num_eval_examples = 1000\n",
    "    eval_batch_size = 200\n",
    "    num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "    print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "    #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "    for restart in range(50):\n",
    "      # Initialize permutation\n",
    "        permutation = np.arange(num_eval_examples)\n",
    "        idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "        Y_test = np.squeeze(Y_test)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "        x_test = X_test \n",
    "        y_test = Y_test\n",
    "        \n",
    "        x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "      # per round\n",
    "        t0 = time.time()\n",
    "        for epoch in range(int(attack.k/10)):\n",
    "            np.random.shuffle(idx)\n",
    "            x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "            permutation = permutation[idx]\n",
    "            \n",
    "            for ibatch in range(num_batches):\n",
    "                bstart = ibatch * eval_batch_size\n",
    "                bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "                x_batch = x_test[bstart:bend, :]\n",
    "                x_batch_adv = x_adv[bstart:bend, :]\n",
    "                y_batch = y_test[bstart:bend]\n",
    "            x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "        inv_permutation = np.argsort(permutation)\n",
    "        x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "        t1 = time.time()\n",
    "\n",
    "        print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "        print('L2: ', np.mean(np.square(x_adv - mnist.test.images)))\n",
    "        print('Linf: ', np.max(np.abs(x_adv - mnist.test.images)))\n",
    "        preds = np.argmax(sess.run(attack.model.predict(tf.convert_to_tensor(x_adv))), 1)\n",
    "        acc = np.mean(np.equal(preds, y_test))\n",
    "        print('adv acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  1.1\n",
      "Iterating over 5 batches\n",
      "inside\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'is_floating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-7336d31e0bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mx_batch_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mx_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-259-7336d31e0bd1>\u001b[0m in \u001b[0;36mperturb\u001b[0;34m(self, x_nat, x_adv, y, sess)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_nat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0mflat_sources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_handle_or_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m         logging.vlog(\n\u001b[1;32m   1001\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the source tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'is_floating'"
     ]
    }
   ],
   "source": [
    "# # DAA 4rd try according to tf2 cleverhans \n",
    "\n",
    "# # WHY IS X_ADV NEGATIVE?\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# c = 1.1\n",
    "# import time\n",
    "\n",
    "# class LinfBLOBAttack:\n",
    "#     def __init__(self, model, epsilon, k, a, random_start):\n",
    "#         self.model = model\n",
    "#         self.epsilon = epsilon\n",
    "#         self.k = k\n",
    "#         self.a = a\n",
    "#         self.rand = random_start\n",
    "\n",
    "#         c = 1.1\n",
    "#         print('c: ', c)\n",
    "\n",
    "#     def perturb(self, x_nat, x_adv, y, sess): \n",
    "#         batch_size = x_adv.shape[0]\n",
    "#         x_adv = tf.convert_to_tensor(x_adv)\n",
    "#         x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#         with tf.GradientTape(persistent=True) as g:\n",
    "#             g.watch(x_adv)\n",
    "# #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "# #             out = tf.expand_dims(out, axis=1)\n",
    "# #             out = tf.dtypes.cast(out, tf.float64)\n",
    "# #             y = tf.dtypes.cast(y, tf.int64)\n",
    "# #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#             out = m4.model(x_adv)\n",
    "#             y = tf.expand_dims(y, axis=1)\n",
    "#             out = tf.dtypes.cast(out, tf.float32)\n",
    "#             y = tf.dtypes.cast(y, tf.int64)\n",
    "#             loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#         print(loss.eval(session=sess))\n",
    "#         for epoch in range(10):\n",
    "#             print(epoch)\n",
    "#             grad = g.gradient(loss, x_adv)\n",
    "#             grad = grad.eval(session=sess)\n",
    "#             x_adv = x_adv.eval(session=sess)\n",
    "#             grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "#             kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "#             x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "#             #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "#             x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "#             x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "# #             print(x_adv)\n",
    "#             x_adv = tf.convert_to_tensor(x_adv)\n",
    "# #             print(x_adv.eval(session=sess))\n",
    "#         return x_adv\n",
    "    \n",
    "#     def svgd_kernel(self, theta):\n",
    "#         theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "#         sq_dist = pdist(theta)\n",
    "#         pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "#         h = np.median(pairwise_dists)  \n",
    "#         h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "#         # compute the rbf kernel\n",
    "#         Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "#         dxkxy = -np.matmul(Kxy, theta)\n",
    "#         sumkxy = np.sum(Kxy, axis=1)\n",
    "#         for i in range(theta.shape[1]):\n",
    "#             dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "#         dxkxy = dxkxy / (h**2)\n",
    "#         return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import json\n",
    "#     import sys\n",
    "#     import math\n",
    "    \n",
    "#     sess =  backend.get_session()\n",
    "\n",
    "#     attack = LinfBLOBAttack(m4,\n",
    "#                          epsilon=0.3,\n",
    "#                          k=200,\n",
    "#                          a=0.01,\n",
    "#                          random_start=True)\n",
    "\n",
    "#     num_eval_examples = 1000\n",
    "#     eval_batch_size = 200\n",
    "#     num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "#     print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "#     #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "#     for restart in range(50):\n",
    "#       # Initialize permutation\n",
    "#         permutation = np.arange(num_eval_examples)\n",
    "#         idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "#         (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#         Y_test = np.squeeze(Y_test)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "#         x_test = X_test \n",
    "#         y_test = Y_test\n",
    "        \n",
    "#         x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "#       # per round\n",
    "#         t0 = time.time()\n",
    "#         for epoch in range(int(attack.k/10)):\n",
    "#             np.random.shuffle(idx)\n",
    "#             x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "#             permutation = permutation[idx]\n",
    "            \n",
    "#             for ibatch in range(num_batches):\n",
    "#                 bstart = ibatch * eval_batch_size\n",
    "#                 bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "#                 x_batch = x_test[bstart:bend, :]\n",
    "#                 x_batch_adv = x_adv[bstart:bend, :]\n",
    "#                 y_batch = y_test[bstart:bend]\n",
    "#             x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "#         inv_permutation = np.argsort(permutation)\n",
    "#         x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "#         t1 = time.time()\n",
    "\n",
    "#         print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "#         print('L2: ', np.mean(np.square(x_adv - mnist.test.images)))\n",
    "#         print('Linf: ', np.max(np.abs(x_adv - mnist.test.images)))\n",
    "#         preds = np.argmax(sess.run(attack.model.predict(tf.convert_to_tensor(x_adv))), 1)\n",
    "#         acc = np.mean(np.equal(preds, y_test))\n",
    "#         print('adv acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODI attack class\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class LinfPGDAttack:\n",
    "    def __init__(self, model, epsilon, num_steps, step_size, random_start, loss_func, batch_size, use_ODI=False):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.rand = random_start\n",
    "        self.batch_size = batch_size\n",
    "        self.use_ODI = use_ODI\n",
    "\n",
    "        if loss_func == 'xent':\n",
    "            loss = model.xent\n",
    "        elif loss_func == 'margin':\n",
    "            label_mask = tf.one_hot(model.y_input,\n",
    "                              10,\n",
    "                              on_value=1.0,\n",
    "                              off_value=0.0,\n",
    "                              dtype=tf.float32)\n",
    "            correct_logit = tf.reduce_sum(label_mask * model.pre_softmax, axis=1)\n",
    "            wrong_logit = tf.reduce_max((1-label_mask) * model.pre_softmax - 1e4*label_mask, axis=1)\n",
    "            loss = wrong_logit - correct_logit\n",
    "      \n",
    "        self.grad = tf.gradients(loss, model.x_input)[0]\n",
    "\n",
    "        if self.use_ODI:\n",
    "            self.rand_direct = tf.Variable(np.zeros((self.batch_size,10)).astype(np.float32),name='rand_direct')\n",
    "            self.input_placeholder = tf.placeholder(tf.float32, shape=[self.batch_size,10])\n",
    "            self.assign_op = self.rand_direct.assign(self.input_placeholder)\n",
    "            loss = tf.tensordot(model.pre_softmax, self.rand_direct,axes=[[0,1],[0,1]])\n",
    "            self.grad_ODI = tf.gradients(loss, model.x_input)[0]\n",
    "\n",
    "    def lossSet(self,rand_vector,sess):\n",
    "        sess.run(self.assign_op,feed_dict={self.input_placeholder: rand_vector.astype(np.float32)})\n",
    "\n",
    "    def perturb(self, x_org, x_start, y, sess):\n",
    "        if self.rand:\n",
    "            x = x_org + np.random.uniform(-self.epsilon, self.epsilon, x_org.shape)\n",
    "            x = np.clip(x, 0, 255) # ensure valid pixel range\n",
    "        else:\n",
    "            x = np.copy(x_start)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            if self.use_ODI:\n",
    "                grad = sess.run(self.grad_ODI, feed_dict={self.model.x_input: x,\n",
    "                                            self.model.y_input: y})\n",
    "            else:\n",
    "                grad = sess.run(self.grad, feed_dict={self.model.x_input: x,\n",
    "                                            self.model.y_input: y})\n",
    "\n",
    "            x = np.add(x, self.step_size * np.sign(grad), out=x, casting='unsafe')\n",
    "\n",
    "            x = np.clip(x, x_org - self.epsilon, x_org + self.epsilon)\n",
    "            x = np.clip(x, 0, 255) # ensure valid pixel range\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "import cifar10_input\n",
    "from model import Model\n",
    "from pgd_attack import LinfPGDAttack\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='CIFAR PGD Attack Evaluation')\n",
    "parser.add_argument('--save_folder', type=str,default='',\n",
    "                    help='name of save folder')\n",
    "parser.add_argument('--data_path', type=str,default='../data/cifar10/',\n",
    "                    help='path of data folder')\n",
    "parser.add_argument('--model_path', type=str,default='./models/model_0/checkpoint-70000',\n",
    "                    help='path of model folder')    \n",
    "parser.add_argument('--evalSize',\n",
    "                    default=10000,type=int,\n",
    "                    help='number of evaluated images')\n",
    "parser.add_argument('--eval_batch_size',\n",
    "                    default=100,type=int,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--num_restart',\n",
    "                    default=20,type=int,\n",
    "                    help='number of restarts')\n",
    "parser.add_argument('--rand_start',\n",
    "                    default=1,type=int,\n",
    "                    help='random initialization for attack')\n",
    "parser.add_argument('--eps',\n",
    "                    default=8.0,type=float,\n",
    "                    help='size of l_inf ball')\n",
    "parser.add_argument('--step_size',\n",
    "                    default=2.0,type=float,\n",
    "                    help='step size for PGD')\n",
    "parser.add_argument('--num_step',\n",
    "                    default=20,type=int,\n",
    "                    help='number of PGD step')\n",
    "parser.add_argument('--step_size_ODI',\n",
    "                    default=8.0,type=float,\n",
    "                    help='step size for ODI')\n",
    "parser.add_argument('--num_step_ODI',\n",
    "                    default=2,type=int,\n",
    "                    help='number of ODI step')                      \n",
    "args = parser.parse_args()\n",
    "\n",
    "cifar = cifar10_input.CIFAR10Data(args.data_path)\n",
    "\n",
    "model = Model(mode='eval')\n",
    "var_all = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "saver = tf.train.Saver(var_all)\n",
    "\n",
    "if args.num_step_ODI > 0:\n",
    "  init_ODI = LinfPGDAttack(model,\n",
    "                        args.eps,\n",
    "                        args.num_step_ODI,\n",
    "                        args.step_size_ODI,\n",
    "                        args.rand_start,\n",
    "                        'margin', \n",
    "                        args.eval_batch_size,\n",
    "                        use_ODI=True)\n",
    "attack_PGD = LinfPGDAttack(model,\n",
    "                      args.eps,\n",
    "                      args.num_step,\n",
    "                      args.step_size,\n",
    "                      False,\n",
    "                      'margin',\n",
    "                      args.eval_batch_size)\n",
    "          \n",
    "          \n",
    "def evaluate_checkpoint(sess):\n",
    "  # Iterate over the samples batch-by-batch\n",
    "  num_batches = int(math.ceil(args.evalSize / args.eval_batch_size))\n",
    "  correct_list = np.ones(args.evalSize)\n",
    "\n",
    "  for ibatch in range(num_batches):\n",
    "    bstart = ibatch * args.eval_batch_size\n",
    "    bend = min(bstart + args.eval_batch_size, args.evalSize)\n",
    "\n",
    "    x_batch = cifar.eval_data.xs[bstart:bend]\n",
    "    y_batch = cifar.eval_data.ys[bstart:bend]\n",
    "    x_batch_org = x_batch\n",
    "    \n",
    "    if args.num_step_ODI > 0:\n",
    "      ran_ = np.random.uniform(-1.0,1.0, (args.eval_batch_size,10))\n",
    "      init_ODI.lossSet(ran_,sess)\n",
    "      x_batch = init_ODI.perturb(x_batch_org,x_batch, y_batch, sess)\n",
    "    elif args.rand_start == 1:\n",
    "      x_batch = x_batch_org + (np.random.uniform(-args.eps, args.eps, x_batch_org.shape) ) \n",
    "      x_batch= np.clip(x_batch, 0., 255.) # ensure valid pixel range\n",
    "\n",
    "    x_batch = attack_PGD.perturb(x_batch_org, x_batch, y_batch, sess)\n",
    "\n",
    "    dict_adv = {model.x_input: x_batch,\n",
    "                model.y_input: y_batch}\n",
    "\n",
    "    cur_isCorrect, = sess.run([model.correct_prediction],\n",
    "                                    feed_dict = dict_adv)\n",
    "    correct_list[bstart:bend] = cur_isCorrect              \n",
    "\n",
    "  return correct_list\n",
    " \n",
    "#main\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, args.model_path)\n",
    "\n",
    "model_dir = 'results/'+args.save_folder\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "is_correct = np.ones(args.evalSize)\n",
    "acc_curve = np.zeros(args.num_restart)\n",
    "for i in range(args.num_restart):\n",
    "    curr_correct = evaluate_checkpoint(sess)\n",
    "    is_correct = curr_correct * is_correct\n",
    "    acc_curve[i] = is_correct.mean()\n",
    "    with open(model_dir+'/result.pk', 'wb') as f:\n",
    "        pickle.dump([acc_curve,is_correct], f)\n",
    "print(acc_curve)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_benign.npy', probs_benign)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_pgd.npy', probs_adv_pgd)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_cw.npy', probs_adv_cw)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_bsa.npy', probs_adv_bsa)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_random.npy', probs_random)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_noise.npy', probs_noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# kernel = stats.gaussian_kde(probs_benign, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(2)\n",
    "# kernel = stats.gaussian_kde(probs_adv_pgd, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(3)\n",
    "# kernel = stats.gaussian_kde(probs_adv_cw, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(4)\n",
    "# kernel = stats.gaussian_kde(probs_adv_bsa, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(5)\n",
    "# kernel = stats.gaussian_kde(probs_random, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(6)\n",
    "# kernel = stats.gaussian_kde(probs_noise, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4.X_test, m4.Y_test y_adv_pgd, X_adv_pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for U=2 we chose randomly the lwta__dense__activation_32 for LogEnsemble for CIFAR10\n",
    "#for U=4 we chose lwta__dense__activation_2 for Tanh16 for CIFAR10\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transform_activations():\n",
    "    acts_l1 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        indices = np.where(y_adv_pgd==i)\n",
    "        adv_input = X_adv_pgd[indices]\n",
    "        probabilities = m4.model_full.get_layer('lwta__dense__activation_5').output[1]\n",
    "        intermediate_layer_model = Model(inputs=m4.model_full.input,outputs=probabilities)\n",
    "        acts_1 = intermediate_layer_model.predict(adv_input) #[1,:,:]\n",
    "\n",
    "        acts_1_mean = acts_1.mean(0)\n",
    "        acts_1 = np.reshape(acts_1_mean, [-1])\n",
    "        acts_l1.append(acts_1)\n",
    "        \n",
    "    return np.array(acts_l1)\n",
    "\n",
    "activations = transform_activations() #change C and U according to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 2 Units \n",
    "\n",
    "def plot_activations(activations, C, U):\n",
    "    \n",
    "    if not os.path.exists('svg'):\n",
    "        os.mkdir('svg')\n",
    "        \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    acts = activations[:,:16]\n",
    "#     acts = activations\n",
    "#     acts = np.expand_dims(acts, axis=1)\n",
    "    ix = plt.imshow(acts, aspect='equal', cmap='binary')\n",
    "    ax = plt.gca()\n",
    "    size = acts.shape[-1]\n",
    "    \n",
    "    ax.set_xticks(np.arange(-.5, size, U))\n",
    "    ax.set_yticks(np.arange(0,10,1))\n",
    "\n",
    "    ax.set_xticklabels(np.arange(0,size+1, U))\n",
    "    ax.set_yticklabels(np.arange(0,10,1))\n",
    "\n",
    "    ax.set_xticks(np.arange(.51, size, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(.51, 10, 1), minor=True);\n",
    "    ax.tick_params(labelsize=14)\n",
    "\n",
    "    ax.grid(which='minor', color=(240/255.,240./255,240/255.), linestyle=':', linewidth=1)\n",
    "    ax.xaxis.grid(True,'major', color='black', linewidth=2)\n",
    "    ax.yaxis.grid(True,'minor')\n",
    "    \n",
    "    ax.annotate('Block 1', xy=(0.07, 1.04), xytext=(0.07, 1.06), xycoords='axes fraction', \n",
    "            fontsize=15, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    ax.annotate('Block 2', xy=(0.19, 1.04), xytext=(0.19, 1.06), xycoords='axes fraction', \n",
    "            fontsize=15, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 3', xy=(0.31, 1.04), xytext=(0.31, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 4', xy=(0.43, 1.04), xytext=(0.43, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 5', xy=(0.56, 1.04), xytext=(0.56, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 6', xy=(0.69, 1.04), xytext=(0.69, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 7', xy=(0.82, 1.04), xytext=(0.82, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 8', xy=(0.94, 1.04), xytext=(0.94, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    plt.xlabel('Units',fontsize=20)\n",
    "    plt.ylabel('Classes',fontsize=20)\n",
    "    \n",
    "    plt.colorbar(fraction=0.025, pad=0.01)\n",
    "    plt.clim(0, 1);\n",
    " \n",
    "#     plt.show()\n",
    "    plt.savefig('svg/cifar_pgd_softmax_U_2_nineth.eps', format='eps',bbox_inches='tight')\n",
    "#         tikz_save(path+'tikz\\\\layer_'+str(i)+'all_digits.tex',  figureheight='20cm', figurewidth='20cm')\n",
    "#     plt.close(fig)\n",
    "\n",
    "# activations_final = []\n",
    "# for i in range(10):\n",
    "#     activations_final.append(activations[i][:20])\n",
    "# activations_final = np.asarray(activations_final)\n",
    "plot_activations(activations=activations, C=10, U=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for 4 Units\n",
    "\n",
    "# def plot_activations(activations, C, U):\n",
    "    \n",
    "#     combs = []\n",
    "#     for i in range(C):\n",
    "#         for j in range(i, C):\n",
    "#             combs.append((i,j))\n",
    "            \n",
    "#     a = list(range(C))\n",
    "#     b = list(range(C))\n",
    "#     combs_2 = list(itertools.product(a, b))\n",
    "    \n",
    "#     if not os.path.exists('svg'):\n",
    "#         os.mkdir('svg')\n",
    "        \n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     acts = activations[:,:16]\n",
    "# #     acts = np.expand_dims(acts, axis=1)\n",
    "#     ix = plt.imshow(acts, aspect='equal', cmap='binary')\n",
    "#     ax = plt.gca()\n",
    "#     size = acts.shape[-1]\n",
    "    \n",
    "#     ax.set_xticks(np.arange(-.5, size, U))\n",
    "#     ax.set_yticks(np.arange(0,10,1))\n",
    "\n",
    "#     ax.set_xticklabels(np.arange(0,size+1, U))\n",
    "#     ax.set_yticklabels(np.arange(0,10,1))\n",
    "\n",
    "#     ax.set_xticks(np.arange(.51, size, 1), minor=True)\n",
    "#     ax.set_yticks(np.arange(.51, 10, 1), minor=True);\n",
    "#     ax.tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "#     ax.grid(which='minor', color=(240/255.,240./255,240/255.), linestyle=':', linewidth=1)\n",
    "#     ax.xaxis.grid(True,'major', color='black', linewidth=2)\n",
    "#     ax.yaxis.grid(True,'minor')\n",
    "    \n",
    "#     ax.annotate('Block 1', xy=(0.13, 1.025), xytext=(0.13, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 2', xy=(0.38, 1.025), xytext=(0.38, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 3', xy=(0.63, 1.025), xytext=(0.63, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 4', xy=(0.88, 1.025), xytext=(0.88, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    \n",
    "#     plt.xlabel('Units',fontsize=20)\n",
    "#     plt.ylabel('Classes',fontsize=20)\n",
    "    \n",
    "#     plt.colorbar(fraction=0.0243, pad=0.01)\n",
    "#     plt.clim(0, 1);\n",
    " \n",
    "# #     plt.show()\n",
    "#     plt.savefig('svg/cifar_pgd_tanh16_U_4.eps', format='eps',bbox_inches='tight')\n",
    "#         #tikz_save(path+'tikz\\\\layer_'+str(i)+'all_digits.tex',  figureheight='20cm', figurewidth='20cm')\n",
    "# #     plt.close(fig)\n",
    "\n",
    "# plot_activations(activations=activations, C=16, U=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver",
   "language": "python",
   "name": "adver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
