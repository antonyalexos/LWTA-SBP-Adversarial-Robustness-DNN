{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/utils_tf.py:345: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Run this to attack a trained model via TrainModel. \n",
    "Use the \"loadFullModel\" submethod to load in an already trained model (trained via TrainModel)\n",
    "The main attack function is \"runAttacks\" which runs attacks on trained models\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from cleverhans.attacks import Noise, CarliniWagnerL2, MaxConfidence, FastGradientMethod, BasicIterativeMethod, DeepFool, MomentumIterativeMethod, ProjectedGradientDescent\n",
    "from Model_Implementations import Model_Softmax_Baseline, Model_Logistic_Baseline, Model_Logistic_Ensemble, Model_Tanh_Ensemble, Model_Tanh_Baseline\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras import backend\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        1664      \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_6 ( ((None, 28, 28, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_7 ( ((None, 14, 14, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_8 ( ((None, 14, 14, 64), (Non 0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_9 ( ((None, 7, 7, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_10  ((None, 7, 7, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "lwta__conv2d__activation_11  ((None, 4, 4, 64), (None, 0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_3 (L ((None, 128), (None, 64,  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_4 (L ((None, 64), (None, 32, 2 0         \n",
      "_________________________________________________________________\n",
      "lwta__dense__activation_5 (L ((None, 64), (None, 32, 2 0         \n",
      "_________________________________________________________________\n",
      "sb__layer_1 (SB_Layer)       (None, 10)                1620      \n",
      "=================================================================\n",
      "Total params: 327,380\n",
      "Trainable params: 327,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/guest/ours/Model.py:296: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Time:  1.5150057151913643e-05\n",
      "\n",
      "\n",
      "\n",
      "Running tests on model:  softmax_baseline_MNIST\n",
      "Clean accuracy of model:\n",
      "Accuracy for model softmax_baseline_MNIST :  0.9908000000000001\n",
      "\n",
      "Benign Time:  7.969145714072511\n",
      "Running PGD attack:\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/attack.py:46: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/utils_tf.py:749: The name tf.assert_greater_equal is deprecated. Please use tf.compat.v1.assert_greater_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/utils_tf.py:740: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/utils_tf.py:477: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/projected_gradient_descent.py:96: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/compat.py:79: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Adv accuracy for model softmax_baseline_MNIST :  0.981\n",
      "\n",
      "PGD Time:  108.21978209295776\n",
      "Running CW attack:\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/carlini_wagner_l2.py:224: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/carlini_wagner_l2.py:262: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/carlini_wagner_l2.py:263: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/carlini_wagner_l2.py:274: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/guest/.local/lib/python3.6/site-packages/cleverhans/attacks/carlini_wagner_l2.py:82: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Adv accuracy for model softmax_baseline_MNIST :  0.98\n",
      "\n",
      "CW Time:  767.2926601070212\n",
      "Running Blind Spot attack, alpha=0.8:\n",
      "Adv accuracy for model softmax_baseline_MNIST :  0.97\n",
      "\n",
      "BSA Time:  823.2500487209763\n",
      "Running random attack:\n",
      "Prob. that  softmax_baseline_MNIST  < 0.9 on random data:  0.962\n",
      "Random Time:  1.370128653012216\n",
      "Running Noise attack:\n",
      "Adv accuracy for model softmax_baseline_MNIST :  0.984\n",
      "\n",
      "Running Noise Time:  13.534457164001651\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models/'  #path with saved model parameters \n",
    "sess =  backend.get_session()\n",
    "backend.set_learning_phase(0) #need to do this to get CleverHans to work with batchnorm\n",
    "\n",
    "\n",
    "#Dataset-specific parameters - should be same as those used in TrainModel\n",
    "\n",
    "#CIFAR\n",
    "# DATA_DESC = 'CIFAR10'; (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "# epochs=None; weight_save_freq=None\n",
    "# num_classes=10  #how many classes (categories) are in this dataset?\n",
    "# Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "# num_filters_std = [32, 64, 128]; num_filters_ens=[32, 64, 128]; num_filters_ens_2=16; dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "# model_rep_baseline=2; model_rep_ens=2; DATA_AUGMENTATION_FLAG=1; BATCH_NORMALIZATION_FLAG=1\n",
    "# num_channels = 3; inp_shape = (32,32,3); lr=1e-4; batch_size=80;\n",
    "# noise_stddev = 0.032; blend_factor = .032\n",
    "\n",
    "#MNIST\n",
    "DATA_DESC = 'MNIST'; (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "num_channels = 1; inp_shape = (28,28,1); num_classes=10\n",
    "#MODEL-SPECIFIC PARAMETERS: MNIST\n",
    "#PARAMETERS RELATED TO SGD OPTIMIZATION\n",
    "epochs=None; weight_save_freq=None; batch_size=80; lr=3e-4; \n",
    "#MODEL DEFINTION PARAMETERS\n",
    "num_filters_std = [64, 64, 64]; num_filters_ens=[32, 32, 32]; num_filters_ens_2=4; \n",
    "dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "noise_stddev = 0.3; blend_factor=0.3; \n",
    "model_rep_baseline=1; model_rep_ens=2; \n",
    "DATA_AUGMENTATION_FLAG=0; BATCH_NORMALIZATION_FLAG=0\n",
    "\n",
    "#Attack parameters\n",
    "eps_val = 8/255.0; PGD_iters = 200; eps_iter=(2/3)*eps_val; \n",
    "eps_range = np.linspace(0, 0.33, 10)\n",
    "noise_eps=0.1\n",
    "\n",
    "\n",
    "# DATA PRE-PROCESSING\n",
    "X_train = (X_train/255).astype(np.float32);  X_test = (X_test/255).astype(np.float32)\n",
    "#reshape (add third (image) channel)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],num_channels); X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "X_valid = X_test[1000:2000]; Y_valid = Y_test[1000:2000]; #validation data, used to attack model\n",
    "X_train = X_train-0.5; X_test = X_test-0.5; X_valid = X_valid-0.5; #map to range (-0.5,0.5)\n",
    "data_dict = {'X_train':X_train, 'Y_train_cat':Y_train, 'X_test':X_test, 'Y_test_cat':Y_test}\n",
    "X_random = np.random.rand(X_valid.shape[0],X_valid.shape[1],X_valid.shape[2],X_valid.shape[3])-0.5; X_random = X_random.astype(np.float32)\n",
    "\n",
    "\n",
    "# def output_activation(x):\n",
    "#     return tf.nn.softmax(x) \n",
    "\n",
    "#Model definition of the model we want to attack; should be same as the definition used in TrainModel\n",
    "name = 'softmax_baseline'+'_'+DATA_DESC; num_chunks=1\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "def output_activation(x):\n",
    "    return tf.nn.softmax(x) \n",
    "base_model=None\n",
    "params_dict = {'weight_decay':weight_decay, 'num_filters_std':num_filters_std, 'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'model_rep':model_rep_baseline, 'base_model':base_model, 'num_chunks':num_chunks, 'output_activation':output_activation,  'batch_size':batch_size, 'epochs':epochs, 'lr':lr, 'dropout_rate':dropout_rate_std,  'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "m4 = Model_Softmax_Baseline(data_dict, params_dict)\n",
    "m4.loadFullModel() #load in the saved model, which should have already been trained first via TrainModel\n",
    "\n",
    "m4.legend = 'TEns16'; \n",
    "m4.X_valid = X_valid; m4.Y_valid = Y_valid; \n",
    "m4.X_test = X_test; m4.Y_test = Y_test; \n",
    "m4.X_random = X_random; \n",
    "m4.minval = -0.5; m4.maxval = 0.5\n",
    "\n",
    "\n",
    "\n",
    "def benignAccuracy(model, X, Y):\n",
    "    \n",
    "    acc_vec=[]; probs_benign_list=[]\n",
    "    for rep in np.arange(0, X.shape[0], 1000):\n",
    "        x = X[rep:rep+1000]\n",
    "        probs_benign = sess.run(model.predict(tf.convert_to_tensor(x))) \n",
    "        acc = np.mean(np.argmax(probs_benign, 1)==Y[rep:rep+1000])\n",
    "        acc_vec += [acc]\n",
    "        probs_benign_list += list(np.max(probs_benign, 1))\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_benign_list\n",
    "\n",
    "\n",
    "def wbAttack(model, attack, att_params, X, Y):\n",
    "    sess =  backend.get_session()\n",
    "    modelCH = model.modelCH()\n",
    "    adv_model = attack(modelCH, sess=sess) \n",
    "    \n",
    "    acc_vec=[]; probs_adv_list=[]\n",
    "    inc=250\n",
    "    for rep in np.arange(0, X.shape[0], inc):\n",
    "        x = X[rep:rep+inc]\n",
    "        y = Y[rep:rep+inc]\n",
    "        X_adv = adv_model.generate(tf.convert_to_tensor(x), **att_params).eval(session=sess)   \n",
    "        preds = np.argmax(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        acc =  np.mean(np.equal(preds, y))\n",
    "        probs_adv = np.max(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        probs_adv = probs_adv[preds != y]\n",
    "        acc= np.mean(np.equal(preds, y))\n",
    "        acc_vec += [acc]\n",
    "        probs_adv_list += list(probs_adv)\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Adv accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_adv_list, acc, X_adv, y\n",
    "\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "\n",
    "\n",
    "def runAttacks(models_list):\n",
    "    #CW attack\n",
    "    for model in models_list:\n",
    "        \n",
    "        print(\"\"); print(\"\"); print(\"\");\n",
    "        print(\"Running tests on model: \", model.params_dict['name'])\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Clean accuracy of model:\")\n",
    "        probs_benign = benignAccuracy(model, model.X_test, model.Y_test)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('Benign Time: ', stop - start)  \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running PGD attack:\")\n",
    "        att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':eps_val, 'eps_iter':eps_iter, 'nb_iter':PGD_iters,'ord':np.inf}\n",
    "        probs_adv_pgd, junk, X_adv_pgd, y_adv_pgd = wbAttack(model, ProjectedGradientDescent, att_params, model.X_valid, model.Y_valid)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('PGD Time: ', stop - start) \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running CW attack:\")\n",
    "        att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "        probs_adv_cw, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, model.X_valid[0:100], model.Y_valid[0:100])\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('CW Time: ', stop - start) \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running Blind Spot attack, alpha=0.8:\")\n",
    "        att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "        probs_adv_bsa, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, 0.8*model.X_valid[0:100], model.Y_valid[0:100])\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('BSA Time: ', stop - start) \n",
    "                \n",
    "        #Random ATTACK (0 SNR inputs)\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running random attack:\")\n",
    "        probs_random = np.max(sess.run(model.predict(tf.convert_to_tensor(model.X_random))), 1)\n",
    "        print('Prob. that ', model.params_dict['name'], ' < 0.9 on random data: ', np.mean(probs_random<0.9))\n",
    "        stop = timeit.default_timer()\n",
    "        print('Random Time: ', stop - start) \n",
    "        \n",
    "        #Noise ATTACK (low SNR inputs)\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Running Noise attack:\")\n",
    "        att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':noise_eps}\n",
    "        probs_noise, junk, X_adv, y = wbAttack(model, Noise, att_params, model.X_valid, model.Y_valid)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('Running Noise Time: ', stop - start) \n",
    "        \n",
    "    return probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd\n",
    "#     return probs_adv_pgd, X_adv_pgd, y_adv_pgd #for plotting probs\n",
    "\n",
    "\n",
    "\n",
    "models_list = [m4]\n",
    "probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd = runAttacks(models_list)\n",
    "# probs_adv_pgd, X_adv_pgd, y_adv_pgd = runAttacks(models_list) #for ploting probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_benign.npy', probs_benign)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_pgd.npy', probs_adv_pgd)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_cw.npy', probs_adv_cw)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_bsa.npy', probs_adv_bsa)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_random.npy', probs_random)\n",
    "# np.save('plots_4/MNIST/tanh_32_diverse_MNIST/tanh_32_diverse_noise.npy', probs_noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# kernel = stats.gaussian_kde(probs_benign, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(2)\n",
    "# kernel = stats.gaussian_kde(probs_adv_pgd, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(3)\n",
    "# kernel = stats.gaussian_kde(probs_adv_cw, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(4)\n",
    "# kernel = stats.gaussian_kde(probs_adv_bsa, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(5)\n",
    "# kernel = stats.gaussian_kde(probs_random, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   \n",
    "\n",
    "# plt.figure(6)\n",
    "# kernel = stats.gaussian_kde(probs_noise, bw_method=0.5)\n",
    "# plt.plot(np.arange(0, 1, .01), kernel.pdf(np.arange(0, 1, .01)), linewidth=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DAA \n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from keras import backend as K\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from matplotlib import pyplot as plt\n",
    "# import os\n",
    "\n",
    "# c = 1.1\n",
    "# import time\n",
    "# class LinfBLOBAttack:\n",
    "#   def __init__(self, model, epsilon, k, a, random_start, loss_func):\n",
    "#     \"\"\"Attack parameter initialization. The attack performs k steps of\n",
    "#        size a, while always staying within epsilon from the initial\n",
    "#        point.\"\"\"\n",
    "#     self.model = model\n",
    "#     self.epsilon = epsilon\n",
    "#     self.k = k\n",
    "#     self.a = a\n",
    "#     self.rand = random_start\n",
    "#     self.loss_func = loss_func\n",
    "    \n",
    "# #     self.grad = tf.gradients(self.model.output, self.model.input)\n",
    "\n",
    "#   def perturb(self, x_nat, x_adv, y, sess):\n",
    "#     \"\"\"Given a set of examples (x_nat, y), returns a set of adversarial\n",
    "#        examples within epsilon of x_nat in l_infinity norm.\"\"\"\n",
    "      \n",
    "#     batch_size = x_adv.shape[0]  \n",
    "    \n",
    "#     for epoch in range(10):\n",
    "# #         print(x_adv.shape, y.shape)\n",
    "\n",
    "# #         with tf.GradientTape() as tape:\n",
    "# #             x_adv = x_adv.astype(np.float32)\n",
    "# #             y = y.astype(np.int32)\n",
    "# #             logits = model(x_adv, training=False)  # Logits for this minibatch\n",
    "# # #             loss=m4.defineLoss(1)\n",
    "# # #             loss_value = loss(y,logits)\n",
    "# #             loss_value = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "# #             grad = tape.gradient(loss_value,model.trainable_weights)\n",
    "#         y = y.astype(np.int32)\n",
    "#         ce = K.tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=self.model.output)\n",
    "# #         ce = tf.keras.backend.binary_crossentropy(y,self.model.output,from_logits=True)\n",
    "#         # compute gradient of loss with respect to inputs\n",
    "#         grad_ce = K.gradients(ce, self.model.input)\n",
    "#         # create a function to be able to run this computation graph\n",
    "#         print(self.model.input)\n",
    "#         func = K.function(self.model.input+[y], grad_ce)\n",
    "\n",
    "#         # usage\n",
    "#         grad = func(x_adv, y)\n",
    "#         grad = tf.reduce_mean(grad)\n",
    "\n",
    "# #             grad = tf.gradients(x_adv,y)\n",
    "#         print(grad)                                                                  \n",
    "#         kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "#         x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "\n",
    "#         x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "#         x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "#     return x_adv\n",
    "\n",
    "    \n",
    "#   def svgd_kernel(self, theta):\n",
    "#     sq_dist = pdist(theta)\n",
    "#     pairwise_dists = squareform(sq_dist)**2\n",
    "    \n",
    "#     h = np.median(pairwise_dists)  \n",
    "#     h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "#     # compute the rbf kernel\n",
    "#     Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "#     dxkxy = -np.matmul(Kxy, theta)\n",
    "#     sumkxy = np.sum(Kxy, axis=1)\n",
    "#     for i in range(theta.shape[1]):\n",
    "#         dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "#     dxkxy = dxkxy / (h**2)\n",
    "#     return (Kxy, dxkxy)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import json\n",
    "#     import sys\n",
    "#     import math\n",
    "\n",
    "#     model = m4.model_full\n",
    "#     attack = LinfBLOBAttack(model, epsilon = 0.30, k = 200, a = 0.01, random_start = True, loss_func = \"xent\")\n",
    "    \n",
    "#     # Iterate over the samples batch-by-batch\n",
    "#     num_eval_examples = 10000\n",
    "#     eval_batch_size = 200\n",
    "#     num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "#     print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "#     x_adv_final = np.copy(X_test)\n",
    "\n",
    "#     for restart in range(50):\n",
    "#         # Initialize permutation\n",
    "#         permutation = np.arange(num_eval_examples)\n",
    "#         idx = np.arange(num_eval_examples)\n",
    "#         # Initialize data\n",
    "#         x_test, y_test = np.copy(X_test), np.copy(Y_test)\n",
    "\n",
    "#         x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "\n",
    "#         # per round\n",
    "#         t0 = time.time()\n",
    "\n",
    "#         for epoch in range(int(attack.k/10)):\n",
    "#             np.random.shuffle(idx)\n",
    "#             x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "#             permutation = permutation[idx]\n",
    "\n",
    "#             for ibatch in range(num_batches):\n",
    "#                 bstart = ibatch * eval_batch_size\n",
    "#                 bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "\n",
    "#                 x_batch = x_test[bstart:bend, :]\n",
    "#                 x_batch_adv = x_adv[bstart:bend, :]\n",
    "#                 y_batch = y_test[bstart:bend]\n",
    "\n",
    "#                 x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "\n",
    "#             inv_permutation = np.argsort(permutation)\n",
    "#             x_adv = x_adv[inv_permutation]\n",
    "\n",
    "#             print('round L2: ', np.mean(np.sqrt(np.sum(np.square(x_adv - mnist.test.images), axis=1))))\n",
    "#             print('round Linf: ', np.max(np.abs(x_adv - mnist.test.images)))\n",
    "#             print('round adv acc: ', sess.run(attack.model.accuracy, feed_dict={attack.model.x_input: x_adv,\n",
    "#                                                 attack.model.y_input: mnist.test.labels}))\n",
    "\n",
    "#             prediction = sess.run(attack.model.correct_prediction, feed_dict={attack.model.x_input: x_adv,\n",
    "#                                                 attack.model.y_input: mnist.test.labels})\n",
    "#             ## Replace with wrong sample\n",
    "#             for i in range(prediction.shape[0]):\n",
    "#                 if not prediction[i]:\n",
    "#                     x_adv_final[i] = x_adv[i]        \n",
    "\n",
    "#                 t1 = time.time()\n",
    "\n",
    "#                 print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "\n",
    "#                 print('L2: ', np.mean(np.square(x_adv_final - mnist.test.images)))\n",
    "#                 print('Linf: ', np.max(np.abs(x_adv_final - mnist.test.images)))\n",
    "\n",
    "#                 print('adv acc: ', sess.run(attack.model.accuracy, feed_dict={attack.model.x_input: x_adv_final,\n",
    "#                                                     attack.model.y_input: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4.X_test, m4.Y_test y_adv_pgd, X_adv_pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for U=2 we chose randomly the lwta__dense__activation_32 for LogEnsemble for CIFAR10\n",
    "#for U=4 we chose lwta__dense__activation_2 for Tanh16 for CIFAR10\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transform_activations():\n",
    "    acts_l1 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        indices = np.where(y_adv_pgd==i)\n",
    "        adv_input = X_adv_pgd[indices]\n",
    "        probabilities = m4.model_full.get_layer('lwta__dense__activation_5').output[1]\n",
    "        intermediate_layer_model = Model(inputs=m4.model_full.input,outputs=probabilities)\n",
    "        acts_1 = intermediate_layer_model.predict(adv_input) #[1,:,:]\n",
    "\n",
    "        acts_1_mean = acts_1.mean(0)\n",
    "        acts_1 = np.reshape(acts_1_mean, [-1])\n",
    "        acts_l1.append(acts_1)\n",
    "        \n",
    "    return np.array(acts_l1)\n",
    "\n",
    "activations = transform_activations() #change C and U according to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 2 Units \n",
    "\n",
    "def plot_activations(activations, C, U):\n",
    "    \n",
    "    if not os.path.exists('svg'):\n",
    "        os.mkdir('svg')\n",
    "        \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    acts = activations[:,:16]\n",
    "#     acts = activations\n",
    "#     acts = np.expand_dims(acts, axis=1)\n",
    "    ix = plt.imshow(acts, aspect='equal', cmap='binary')\n",
    "    ax = plt.gca()\n",
    "    size = acts.shape[-1]\n",
    "    \n",
    "    ax.set_xticks(np.arange(-.5, size, U))\n",
    "    ax.set_yticks(np.arange(0,10,1))\n",
    "\n",
    "    ax.set_xticklabels(np.arange(0,size+1, U))\n",
    "    ax.set_yticklabels(np.arange(0,10,1))\n",
    "\n",
    "    ax.set_xticks(np.arange(.51, size, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(.51, 10, 1), minor=True);\n",
    "    ax.tick_params(labelsize=14)\n",
    "\n",
    "    ax.grid(which='minor', color=(240/255.,240./255,240/255.), linestyle=':', linewidth=1)\n",
    "    ax.xaxis.grid(True,'major', color='black', linewidth=2)\n",
    "    ax.yaxis.grid(True,'minor')\n",
    "    \n",
    "    ax.annotate('Block 1', xy=(0.07, 1.04), xytext=(0.07, 1.06), xycoords='axes fraction', \n",
    "            fontsize=15, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    ax.annotate('Block 2', xy=(0.19, 1.04), xytext=(0.19, 1.06), xycoords='axes fraction', \n",
    "            fontsize=15, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 3', xy=(0.31, 1.04), xytext=(0.31, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 4', xy=(0.43, 1.04), xytext=(0.43, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 5', xy=(0.56, 1.04), xytext=(0.56, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 6', xy=(0.69, 1.04), xytext=(0.69, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 7', xy=(0.82, 1.04), xytext=(0.82, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "    ax.annotate('Block 8', xy=(0.94, 1.04), xytext=(0.94, 1.06), xycoords='axes fraction', \n",
    "        fontsize=15, ha='center', va='bottom',\n",
    "        bbox=dict(boxstyle='square', fc='white'),\n",
    "        arrowprops=dict(arrowstyle='-[, widthB=2, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    plt.xlabel('Units',fontsize=20)\n",
    "    plt.ylabel('Classes',fontsize=20)\n",
    "    \n",
    "    plt.colorbar(fraction=0.025, pad=0.01)\n",
    "    plt.clim(0, 1);\n",
    " \n",
    "#     plt.show()\n",
    "    plt.savefig('svg/cifar_pgd_softmax_U_2_nineth.eps', format='eps',bbox_inches='tight')\n",
    "#         tikz_save(path+'tikz\\\\layer_'+str(i)+'all_digits.tex',  figureheight='20cm', figurewidth='20cm')\n",
    "#     plt.close(fig)\n",
    "\n",
    "# activations_final = []\n",
    "# for i in range(10):\n",
    "#     activations_final.append(activations[i][:20])\n",
    "# activations_final = np.asarray(activations_final)\n",
    "plot_activations(activations=activations, C=10, U=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for 4 Units\n",
    "\n",
    "# def plot_activations(activations, C, U):\n",
    "    \n",
    "#     combs = []\n",
    "#     for i in range(C):\n",
    "#         for j in range(i, C):\n",
    "#             combs.append((i,j))\n",
    "            \n",
    "#     a = list(range(C))\n",
    "#     b = list(range(C))\n",
    "#     combs_2 = list(itertools.product(a, b))\n",
    "    \n",
    "#     if not os.path.exists('svg'):\n",
    "#         os.mkdir('svg')\n",
    "        \n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     acts = activations[:,:16]\n",
    "# #     acts = np.expand_dims(acts, axis=1)\n",
    "#     ix = plt.imshow(acts, aspect='equal', cmap='binary')\n",
    "#     ax = plt.gca()\n",
    "#     size = acts.shape[-1]\n",
    "    \n",
    "#     ax.set_xticks(np.arange(-.5, size, U))\n",
    "#     ax.set_yticks(np.arange(0,10,1))\n",
    "\n",
    "#     ax.set_xticklabels(np.arange(0,size+1, U))\n",
    "#     ax.set_yticklabels(np.arange(0,10,1))\n",
    "\n",
    "#     ax.set_xticks(np.arange(.51, size, 1), minor=True)\n",
    "#     ax.set_yticks(np.arange(.51, 10, 1), minor=True);\n",
    "#     ax.tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "#     ax.grid(which='minor', color=(240/255.,240./255,240/255.), linestyle=':', linewidth=1)\n",
    "#     ax.xaxis.grid(True,'major', color='black', linewidth=2)\n",
    "#     ax.yaxis.grid(True,'minor')\n",
    "    \n",
    "#     ax.annotate('Block 1', xy=(0.13, 1.025), xytext=(0.13, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 2', xy=(0.38, 1.025), xytext=(0.38, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 3', xy=(0.63, 1.025), xytext=(0.63, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "    \n",
    "#     ax.annotate('Block 4', xy=(0.88, 1.025), xytext=(0.88, 1.07), xycoords='axes fraction', \n",
    "#             fontsize=10, ha='center', va='bottom',\n",
    "#             bbox=dict(boxstyle='square', fc='white'),\n",
    "#             arrowprops=dict(arrowstyle='-[, widthB=8, lengthB=1.5', lw=2.0))\n",
    "\n",
    "    \n",
    "#     plt.xlabel('Units',fontsize=20)\n",
    "#     plt.ylabel('Classes',fontsize=20)\n",
    "    \n",
    "#     plt.colorbar(fraction=0.0243, pad=0.01)\n",
    "#     plt.clim(0, 1);\n",
    " \n",
    "# #     plt.show()\n",
    "#     plt.savefig('svg/cifar_pgd_tanh16_U_4.eps', format='eps',bbox_inches='tight')\n",
    "#         #tikz_save(path+'tikz\\\\layer_'+str(i)+'all_digits.tex',  figureheight='20cm', figurewidth='20cm')\n",
    "# #     plt.close(fig)\n",
    "\n",
    "# plot_activations(activations=activations, C=16, U=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver",
   "language": "python",
   "name": "adver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
