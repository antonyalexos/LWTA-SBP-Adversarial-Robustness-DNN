{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Run this to attack a trained model via TrainModel. \n",
    "Use the \"loadFullModel\" submethod to load in an already trained model (trained via TrainModel)\n",
    "The main attack function is \"runAttacks\" which runs attacks on trained models\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "from cleverhans.attacks import Noise, CarliniWagnerL2, MaxConfidence, FastGradientMethod, BasicIterativeMethod, DeepFool, MomentumIterativeMethod, ProjectedGradientDescent\n",
    "from Model_Implementations import Model_Softmax_Baseline, Model_Logistic_Baseline, Model_Logistic_Ensemble, Model_Tanh_Ensemble, Model_Tanh_Baseline\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras import backend\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = 'models/'  #path with saved model parameters \n",
    "sess =  backend.get_session()\n",
    "backend.set_learning_phase(0) #need to do this to get CleverHans to work with batchnorm\n",
    "\n",
    "\n",
    "#Dataset-specific parameters - should be same as those used in TrainModel\n",
    "\n",
    "#CIFAR\n",
    "# DATA_DESC = 'CIFAR10'; (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "# epochs=None; weight_save_freq=None\n",
    "# num_classes=10  #how many classes (categories) are in this dataset?\n",
    "# Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "# num_filters_std = [32, 64, 128]; num_filters_ens=[32, 64, 128]; num_filters_ens_2=16; dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "# model_rep_baseline=2; model_rep_ens=2; DATA_AUGMENTATION_FLAG=1; BATCH_NORMALIZATION_FLAG=1\n",
    "# num_channels = 3; inp_shape = (32,32,3); lr=1e-4; batch_size=80;\n",
    "# noise_stddev = 0.032; blend_factor = .032\n",
    "\n",
    "#MNIST\n",
    "DATA_DESC = 'MNIST'; (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "num_channels = 1; inp_shape = (28,28,1); num_classes=10\n",
    "#MODEL-SPECIFIC PARAMETERS: MNIST\n",
    "#PARAMETERS RELATED TO SGD OPTIMIZATION\n",
    "epochs=None; weight_save_freq=None; batch_size=80; lr=3e-4; \n",
    "#MODEL DEFINTION PARAMETERS\n",
    "num_filters_std = [64, 64, 64]; num_filters_ens=[32, 32, 32]; num_filters_ens_2=4; \n",
    "dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "noise_stddev = 0.3; blend_factor=0.3; \n",
    "model_rep_baseline=1; model_rep_ens=2; \n",
    "DATA_AUGMENTATION_FLAG=0; BATCH_NORMALIZATION_FLAG=0\n",
    "\n",
    "#Attack parameters\n",
    "eps_val = 8/255.0; PGD_iters = 200; eps_iter=(2/3)*eps_val; \n",
    "eps_range = np.linspace(0, 0.33, 10)\n",
    "noise_eps=0.1\n",
    "\n",
    "\n",
    "# DATA PRE-PROCESSING\n",
    "X_train = (X_train/255).astype(np.float32);  X_test = (X_test/255).astype(np.float32)\n",
    "#reshape (add third (image) channel)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],num_channels); X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "X_valid = X_test[1000:2000]; Y_valid = Y_test[1000:2000]; #validation data, used to attack model\n",
    "X_train = X_train-0.5; X_test = X_test-0.5; X_valid = X_valid-0.5; #map to range (-0.5,0.5)\n",
    "data_dict = {'X_train':X_train, 'Y_train_cat':Y_train, 'X_test':X_test, 'Y_test_cat':Y_test}\n",
    "X_random = np.random.rand(X_valid.shape[0],X_valid.shape[1],X_valid.shape[2],X_valid.shape[3])-0.5; X_random = X_random.astype(np.float32)\n",
    "\n",
    "\n",
    "# def output_activation(x):\n",
    "#     return tf.nn.softmax(x) \n",
    "name = 'logistic_diverse'+'_'+DATA_DESC; num_chunks=2\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "base_model=None\n",
    "def output_activation(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "params_dict = {'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'base_model':base_model, 'num_chunks':num_chunks, 'model_rep': model_rep_ens, 'output_activation':output_activation, 'num_filters_ens':num_filters_ens, 'num_filters_ens_2':num_filters_ens_2,'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_ens,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "m4 = Model_Logistic_Ensemble(data_dict, params_dict)\n",
    "m4.loadFullModel()\n",
    "\n",
    "#Model definition of the model we want to attack; should be same as the definition used in TrainModel\n",
    "# name = 'tanh_16_diverse'+'_'+DATA_DESC; seed = 59; code_length=16; num_codes=code_length; num_chunks=4; base_model=None; \n",
    "# def output_activation(x):\n",
    "#     return tf.nn.tanh(x)\n",
    "# M = scipy.linalg.hadamard(code_length).astype(np.float32)\n",
    "# M[np.arange(0, num_codes,2), 0]= -1#replace first col, which for scipy's Hadamard construction is always 1, hence not a useful classifier; this change still ensures all codewords have dot product <=0; since our decoder ignores negative correlations anyway, this has no net effect on probability estimation\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(M)\n",
    "# idx=np.random.permutation(code_length)\n",
    "# M = M[0:num_codes, idx[0:code_length]]\n",
    "# params_dict = {'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'base_model':base_model, 'num_chunks':num_chunks, 'model_rep': model_rep_ens, 'output_activation':output_activation, 'num_filters_ens':num_filters_ens, 'num_filters_ens_2':num_filters_ens_2,'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_ens,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "# m4 = Model_Tanh_Ensemble(data_dict, params_dict)\n",
    "# m4.loadFullModel() #load in the saved model, which should have already been trained first via TrainModel\n",
    "\n",
    "m4.legend = 'TEns16'; \n",
    "m4.X_valid = X_valid; m4.Y_valid = Y_valid; \n",
    "m4.X_test = X_test; m4.Y_test = Y_test; \n",
    "m4.X_random = X_random; \n",
    "m4.minval = -0.5; m4.maxval = 0.5\n",
    "\n",
    "\n",
    "\n",
    "def benignAccuracy(model, X, Y):\n",
    "    \n",
    "    acc_vec=[]; probs_benign_list=[]\n",
    "    for rep in np.arange(0, X.shape[0], 1000):\n",
    "        x = X[rep:rep+1000]\n",
    "        probs_benign = sess.run(model.predict(tf.convert_to_tensor(x))) \n",
    "        acc = np.mean(np.argmax(probs_benign, 1)==Y[rep:rep+1000])\n",
    "        acc_vec += [acc]\n",
    "        probs_benign_list += list(np.max(probs_benign, 1))\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_benign_list\n",
    "\n",
    "\n",
    "def wbAttack(model, attack, att_params, X, Y):\n",
    "    sess =  backend.get_session()\n",
    "    modelCH = model.modelCH()\n",
    "    adv_model = attack(modelCH, sess=sess) \n",
    "    \n",
    "    acc_vec=[]; probs_adv_list=[]\n",
    "    inc=250\n",
    "    for rep in np.arange(0, X.shape[0], inc):\n",
    "        x = X[rep:rep+inc]\n",
    "        y = Y[rep:rep+inc]\n",
    "        X_adv = adv_model.generate(tf.convert_to_tensor(x), **att_params).eval(session=sess)  \n",
    "        print(X_adv.dtype)\n",
    "        preds = np.argmax(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        acc =  np.mean(np.equal(preds, y))\n",
    "        probs_adv = np.max(sess.run(model.predict(tf.convert_to_tensor(X_adv))), 1)\n",
    "        probs_adv = probs_adv[preds != y]\n",
    "        acc= np.mean(np.equal(preds, y))\n",
    "        acc_vec += [acc]\n",
    "        probs_adv_list += list(probs_adv)\n",
    "\n",
    "        \n",
    "    acc = np.mean(acc_vec)        \n",
    "    print(\"Adv accuracy for model \" + model.params_dict['name'] + \" : \", acc)    \n",
    "    return probs_adv_list, acc, X_adv, y\n",
    "\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "\n",
    "\n",
    "def runAttacks(models_list):\n",
    "    #CW attack\n",
    "    for model in models_list:\n",
    "        \n",
    "        print(\"\"); print(\"\"); print(\"\");\n",
    "        print(\"Running tests on model: \", model.params_dict['name'])\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Clean accuracy of model:\")\n",
    "        probs_benign = benignAccuracy(model, model.X_test, model.Y_test)\n",
    "        print(\"\")\n",
    "        stop = timeit.default_timer()\n",
    "        print('Benign Time: ', stop - start)  \n",
    "        \n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running PGD attack:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':eps_val, 'eps_iter':eps_iter, 'nb_iter':PGD_iters,'ord':np.inf}\n",
    "#         probs_adv_pgd, junk, X_adv_pgd, y_adv_pgd = wbAttack(model, ProjectedGradientDescent, att_params, model.X_valid, model.Y_valid)\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('PGD Time: ', stop - start) \n",
    "        \n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running CW attack:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "#         probs_adv_cw, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, model.X_valid[0:100], model.Y_valid[0:100])\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('CW Time: ', stop - start) \n",
    "        \n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running Blind Spot attack, alpha=0.8:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval,  'binary_search_steps':10, 'learning_rate':1e-3}\n",
    "#         probs_adv_bsa, junk, X_adv, y = wbAttack(model, CarliniWagnerL2, att_params, 0.8*model.X_valid[0:100], model.Y_valid[0:100])\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('BSA Time: ', stop - start) \n",
    "                \n",
    "        #Random ATTACK (0 SNR inputs)\n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running random attack:\")\n",
    "#         probs_random = np.max(sess.run(model.predict(tf.convert_to_tensor(model.X_random))), 1)\n",
    "#         print('Prob. that ', model.params_dict['name'], ' < 0.9 on random data: ', np.mean(probs_random<0.9))\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('Random Time: ', stop - start) \n",
    "        \n",
    "#         #Noise ATTACK (low SNR inputs)\n",
    "#         start = timeit.default_timer()\n",
    "#         print(\"Running Noise attack:\")\n",
    "#         att_params = {'clip_min': model.minval, 'clip_max':model.maxval, 'eps':noise_eps}\n",
    "#         probs_noise, junk, X_adv, y = wbAttack(model, Noise, att_params, model.X_valid, model.Y_valid)\n",
    "#         print(\"\")\n",
    "#         stop = timeit.default_timer()\n",
    "#         print('Running Noise Time: ', stop - start) \n",
    "        \n",
    "#     return probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd\n",
    "#     return probs_adv_pgd, X_adv_pgd, y_adv_pgd #for plotting probs\n",
    "\n",
    "\n",
    "\n",
    "models_list = [m4]\n",
    "# probs_benign, probs_adv_pgd, probs_adv_cw, probs_adv_bsa, probs_random, probs_noise, X_adv_pgd, y_adv_pgd = runAttacks(models_list)\n",
    "# probs_adv_pgd, X_adv_pgd, y_adv_pgd = runAttacks(models_list) #for ploting probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAA for Logistic Ensemble models\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "c = 1.1\n",
    "import time\n",
    "\n",
    "class LinfBLOBAttack:\n",
    "    def __init__(self, model, epsilon, k, a, random_start):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self.a = a\n",
    "        self.rand = random_start\n",
    "\n",
    "        c = 1.1\n",
    "        print('c: ', c)\n",
    "\n",
    "    def perturb(self, x_nat, x_adv, y, sess): \n",
    "        batch_size = x_adv.shape[0]\n",
    "        y = tf.expand_dims(y, axis=1)\n",
    "#         y = tf.expand_dims(y, axis=1)\n",
    "        y = tf.dtypes.cast(y, tf.float32)\n",
    "        for epoch in range(10):\n",
    "#             print(epoch)\n",
    "            x_adv = tf.convert_to_tensor(x_adv)\n",
    "            x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "            with tf.GradientTape(persistent=True) as g:\n",
    "                g.watch(x_adv)\n",
    "    #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "    #             out = tf.expand_dims(out, axis=1)\n",
    "    #             out = tf.dtypes.cast(out, tf.float64)\n",
    "    #             y = tf.dtypes.cast(y, tf.int64)\n",
    "    #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "                out = m4.model(x_adv)\n",
    "                out = tf.dtypes.cast(out, tf.float32)\n",
    "#                 loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "                loss = [m4.defineLoss(k)(y,out) for k in np.arange(m4.params_dict['num_chunks'])]\n",
    "#                 loss = tf.reduce_mean(loss,0)\n",
    "\n",
    "            grad = g.gradient(loss, x_adv)\n",
    "            grad = grad.eval(session=sess)\n",
    "            x_adv = x_adv.eval(session=sess)\n",
    "            grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "            kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "            x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "            x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "            #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "            x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "            x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "            x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "        return x_adv\n",
    "    \n",
    "    def svgd_kernel(self, theta):\n",
    "        theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "        h = np.median(pairwise_dists)  \n",
    "        h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import json\n",
    "    import sys\n",
    "    import math\n",
    "    \n",
    "    sess =  backend.get_session()\n",
    "\n",
    "    attack = LinfBLOBAttack(m4,\n",
    "                         epsilon=0.3,\n",
    "                         k=200,\n",
    "                         a=0.01,\n",
    "                         random_start=True)\n",
    "\n",
    "    num_eval_examples = 1000\n",
    "    eval_batch_size = 200\n",
    "    num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "    print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "    #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "    for restart in range(50):\n",
    "      # Initialize permutation\n",
    "        permutation = np.arange(num_eval_examples)\n",
    "        idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "        Y_test = np.squeeze(Y_test)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "        x_test = X_test \n",
    "        y_test = Y_test\n",
    "        \n",
    "        x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "      # per round\n",
    "        t0 = time.time()\n",
    "        for epoch in range(int(attack.k/10)):\n",
    "            print(epoch)\n",
    "            print(int(attack.k/10))\n",
    "            np.random.shuffle(idx)\n",
    "            x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "            permutation = permutation[idx]\n",
    "            \n",
    "            for ibatch in range(num_batches):\n",
    "                bstart = ibatch * eval_batch_size\n",
    "                bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "                x_batch = x_test[bstart:bend, :]\n",
    "                x_batch_adv = x_adv[bstart:bend, :]\n",
    "                y_batch = y_test[bstart:bend]\n",
    "            x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "        inv_permutation = np.argsort(permutation)\n",
    "        x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "        t1 = time.time()\n",
    "\n",
    "        print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "        print('L2: ', np.mean(np.square(x_adv - x_test)))\n",
    "        print('Linf: ', np.max(np.abs(x_adv - x_test)))\n",
    "        x_adv = tf.convert_to_tensor(x_adv)\n",
    "        x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "        preds = np.argmax(sess.run(attack.model.predict(x_adv)), 1)\n",
    "        acc = np.mean(np.equal(preds, y_test))\n",
    "        print('adv acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # DAA for Tahn16 Ensemble models\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# c = 1.1\n",
    "# import time\n",
    "\n",
    "# class LinfBLOBAttack:\n",
    "#     def __init__(self, model, epsilon, k, a, random_start):\n",
    "#         self.model = model\n",
    "#         self.epsilon = epsilon\n",
    "#         self.k = k\n",
    "#         self.a = a\n",
    "#         self.rand = random_start\n",
    "\n",
    "#         c = 1.1\n",
    "#         print('c: ', c)\n",
    "\n",
    "#     def perturb(self, x_nat, x_adv, y, sess): \n",
    "#         batch_size = x_adv.shape[0]\n",
    "#         y = tf.expand_dims(y, axis=1)\n",
    "#         y = tf.dtypes.cast(y, tf.float32)\n",
    "#         for epoch in range(10):\n",
    "# #             print(epoch)\n",
    "#             x_adv = tf.convert_to_tensor(x_adv)\n",
    "#             x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#             with tf.GradientTape(persistent=True) as g:\n",
    "#                 g.watch(x_adv)\n",
    "#     #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "#     #             out = tf.expand_dims(out, axis=1)\n",
    "#     #             out = tf.dtypes.cast(out, tf.float64)\n",
    "#     #             y = tf.dtypes.cast(y, tf.int64)\n",
    "#     #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 out = m4.model(x_adv)\n",
    "#                 out = tf.dtypes.cast(out, tf.float32)\n",
    "# #                 loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 loss = [m4.defineLoss(k)(y,out) for k in np.arange(m4.params_dict['num_chunks'])]\n",
    "# #                 loss = tf.reduce_mean(loss,0)\n",
    "\n",
    "#             grad = g.gradient(loss, x_adv)\n",
    "#             grad = grad.eval(session=sess)\n",
    "#             x_adv = x_adv.eval(session=sess)\n",
    "#             grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "#             kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "#             x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "#             #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "#             x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "#             x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "#         return x_adv\n",
    "    \n",
    "#     def svgd_kernel(self, theta):\n",
    "#         theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "#         sq_dist = pdist(theta)\n",
    "#         pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "#         h = np.median(pairwise_dists)  \n",
    "#         h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "#         # compute the rbf kernel\n",
    "#         Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "#         dxkxy = -np.matmul(Kxy, theta)\n",
    "#         sumkxy = np.sum(Kxy, axis=1)\n",
    "#         for i in range(theta.shape[1]):\n",
    "#             dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "#         dxkxy = dxkxy / (h**2)\n",
    "#         return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import json\n",
    "#     import sys\n",
    "#     import math\n",
    "    \n",
    "#     sess =  backend.get_session()\n",
    "\n",
    "#     attack = LinfBLOBAttack(m4,\n",
    "#                          epsilon=0.3,\n",
    "#                          k=200,\n",
    "#                          a=0.01,\n",
    "#                          random_start=True)\n",
    "\n",
    "#     num_eval_examples = 1000\n",
    "#     eval_batch_size = 200\n",
    "#     num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "#     print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "#     #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "#     for restart in range(50):\n",
    "#       # Initialize permutation\n",
    "#         permutation = np.arange(num_eval_examples)\n",
    "#         idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "#         (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#         Y_test = np.squeeze(Y_test)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "#         x_test = X_test \n",
    "#         y_test = Y_test\n",
    "        \n",
    "#         x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "#       # per round\n",
    "#         t0 = time.time()\n",
    "#         for epoch in range(int(attack.k/10)):\n",
    "#             print(epoch)\n",
    "#             print(int(attack.k/10))\n",
    "#             np.random.shuffle(idx)\n",
    "#             x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "#             permutation = permutation[idx]\n",
    "            \n",
    "#             for ibatch in range(num_batches):\n",
    "#                 bstart = ibatch * eval_batch_size\n",
    "#                 bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "#                 x_batch = x_test[bstart:bend, :]\n",
    "#                 x_batch_adv = x_adv[bstart:bend, :]\n",
    "#                 y_batch = y_test[bstart:bend]\n",
    "#             x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "#         inv_permutation = np.argsort(permutation)\n",
    "#         x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "#         t1 = time.time()\n",
    "\n",
    "#         print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "#         print('L2: ', np.mean(np.square(x_adv - x_test)))\n",
    "#         print('Linf: ', np.max(np.abs(x_adv - x_test)))\n",
    "#         x_adv = tf.convert_to_tensor(x_adv)\n",
    "#         x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#         preds = np.argmax(sess.run(attack.model.predict(x_adv)), 1)\n",
    "#         acc = np.mean(np.equal(preds, y_test))\n",
    "#         print('adv acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DAA for Tahn16 Standard models\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# c = 1.1\n",
    "# import time\n",
    "\n",
    "# class LinfBLOBAttack:\n",
    "#     def __init__(self, model, epsilon, k, a, random_start):\n",
    "#         self.model = model\n",
    "#         self.epsilon = epsilon\n",
    "#         self.k = k\n",
    "#         self.a = a\n",
    "#         self.rand = random_start\n",
    "\n",
    "#         c = 1.1\n",
    "#         print('c: ', c)\n",
    "\n",
    "#     def perturb(self, x_nat, x_adv, y, sess): \n",
    "#         batch_size = x_adv.shape[0]\n",
    "#         for epoch in range(10):\n",
    "# #             print(epoch)\n",
    "#             x_adv = tf.convert_to_tensor(x_adv)\n",
    "#             x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#             with tf.GradientTape(persistent=True) as g:\n",
    "#                 g.watch(x_adv)\n",
    "#     #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "#     #             out = tf.expand_dims(out, axis=1)\n",
    "#     #             out = tf.dtypes.cast(out, tf.float64)\n",
    "#     #             y = tf.dtypes.cast(y, tf.int64)\n",
    "#     #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 out = m4.model(x_adv)\n",
    "#                 # the next 2 lines are for the sigmoid!!!!!\n",
    "# #                 out = tf.math.argmax(out, 1)\n",
    "# #                 out = tf.expand_dims(out, axis=1)\n",
    "                \n",
    "#                 y = tf.expand_dims(y, axis=1)\n",
    "#                 out = tf.dtypes.cast(out, tf.float32)\n",
    "#                 #for sigmoid we need float32\n",
    "#                 y = tf.dtypes.cast(y, tf.float32)\n",
    "# #                 y = tf.dtypes.cast(y, tf.int64)\n",
    "# #                 loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 loss = [m4.defineLoss(k)(y,out) for k in np.arange(m4.params_dict['num_chunks'])]\n",
    "\n",
    "#             grad = g.gradient(loss, x_adv)\n",
    "#             grad = grad.eval(session=sess)\n",
    "#             x_adv = x_adv.eval(session=sess)\n",
    "#             grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "#             kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "#             x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "#             #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "#             x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "#             x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "#         return x_adv\n",
    "    \n",
    "#     def svgd_kernel(self, theta):\n",
    "#         theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "#         sq_dist = pdist(theta)\n",
    "#         pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "#         h = np.median(pairwise_dists)  \n",
    "#         h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "#         # compute the rbf kernel\n",
    "#         Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "#         dxkxy = -np.matmul(Kxy, theta)\n",
    "#         sumkxy = np.sum(Kxy, axis=1)\n",
    "#         for i in range(theta.shape[1]):\n",
    "#             dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "#         dxkxy = dxkxy / (h**2)\n",
    "#         return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import json\n",
    "#     import sys\n",
    "#     import math\n",
    "    \n",
    "#     sess =  backend.get_session()\n",
    "\n",
    "#     attack = LinfBLOBAttack(m4,\n",
    "#                          epsilon=0.3,\n",
    "#                          k=200,\n",
    "#                          a=0.01,\n",
    "#                          random_start=True)\n",
    "\n",
    "#     num_eval_examples = 1000\n",
    "#     eval_batch_size = 200\n",
    "#     num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "#     print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "#     #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "#     for restart in range(50):\n",
    "#       # Initialize permutation\n",
    "#         permutation = np.arange(num_eval_examples)\n",
    "#         idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "#         (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#         Y_test = np.squeeze(Y_test)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "#         x_test = X_test \n",
    "#         y_test = Y_test\n",
    "        \n",
    "#         x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "#       # per round\n",
    "#         t0 = time.time()\n",
    "#         for epoch in range(int(attack.k/10)):\n",
    "#             print(epoch)\n",
    "#             np.random.shuffle(idx)\n",
    "#             x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "#             permutation = permutation[idx]\n",
    "            \n",
    "#             for ibatch in range(num_batches):\n",
    "#                 bstart = ibatch * eval_batch_size\n",
    "#                 bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "#                 x_batch = x_test[bstart:bend, :]\n",
    "#                 x_batch_adv = x_adv[bstart:bend, :]\n",
    "#                 y_batch = y_test[bstart:bend]\n",
    "#             x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "#         inv_permutation = np.argsort(permutation)\n",
    "#         x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "#         t1 = time.time()\n",
    "\n",
    "#         print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "#         print('L2: ', np.mean(np.square(x_adv - x_test)))\n",
    "#         print('Linf: ', np.max(np.abs(x_adv - x_test)))\n",
    "#         x_adv = tf.convert_to_tensor(x_adv)\n",
    "#         x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#         preds = np.argmax(sess.run(attack.model.predict(x_adv)), 1)\n",
    "#         acc = np.mean(np.equal(preds, y_test))\n",
    "#         print('adv acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # DAA for Softmax Standard models\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# c = 1.1\n",
    "# import time\n",
    "\n",
    "# class LinfBLOBAttack:\n",
    "#     def __init__(self, model, epsilon, k, a, random_start):\n",
    "#         self.model = model\n",
    "#         self.epsilon = epsilon\n",
    "#         self.k = k\n",
    "#         self.a = a\n",
    "#         self.rand = random_start\n",
    "\n",
    "#         c = 1.1\n",
    "#         print('c: ', c)\n",
    "\n",
    "#     def perturb(self, x_nat, x_adv, y, sess): \n",
    "#         batch_size = x_adv.shape[0]\n",
    "#         for epoch in range(10):\n",
    "# #             print(epoch)\n",
    "#             x_adv = tf.convert_to_tensor(x_adv)\n",
    "#             x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#             with tf.GradientTape(persistent=True) as g:\n",
    "#                 g.watch(x_adv)\n",
    "#     #             out = tf.argmax(m4.model(x_nat),axis=1)\n",
    "#     #             out = tf.expand_dims(out, axis=1)\n",
    "#     #             out = tf.dtypes.cast(out, tf.float64)\n",
    "#     #             y = tf.dtypes.cast(y, tf.int64)\n",
    "#     #             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 out = m4.model(x_adv)\n",
    "#                 y = tf.expand_dims(y, axis=1)\n",
    "#                 out = tf.dtypes.cast(out, tf.float32)\n",
    "#                 y = tf.dtypes.cast(y, tf.int64)\n",
    "# #                 loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "#                 loss = [m4.defineLoss(k)(y,out) for k in np.arange(m4.params_dict['num_chunks'])]\n",
    "\n",
    "#             grad = g.gradient(loss, x_adv)\n",
    "#             grad = grad.eval(session=sess)\n",
    "#             x_adv = x_adv.eval(session=sess)\n",
    "#             grad = np.reshape(grad, [grad.shape[0], grad.shape[1]*grad.shape[2]*grad.shape[3]])\n",
    "#             kxy, dxkxy = self.svgd_kernel(x_adv)\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], x_adv.shape[1]*x_adv.shape[2]*x_adv.shape[3]])\n",
    "#             x_adv += self.a * np.sign(c*(-(np.matmul(kxy, -grad) + dxkxy)/batch_size) + grad)\n",
    "            \n",
    "#             #we need to put back the dimensions since our model has a 4-dimensional input\n",
    "#             x_adv = np.reshape(x_adv, [x_adv.shape[0], 28, 28, 1])\n",
    "            \n",
    "#             x_adv = np.clip(x_adv, x_nat - self.epsilon, x_nat + self.epsilon) \n",
    "            \n",
    "#             x_adv = np.clip(x_adv, 0, 1) # ensure valid pixel range\n",
    "\n",
    "#         return x_adv\n",
    "    \n",
    "#     def svgd_kernel(self, theta):\n",
    "#         theta = np.reshape(theta, [theta.shape[0], theta.shape[1]*theta.shape[2]])\n",
    "#         sq_dist = pdist(theta)\n",
    "#         pairwise_dists = squareform(sq_dist)**2\n",
    "\n",
    "#         h = np.median(pairwise_dists)  \n",
    "#         h = np.sqrt(0.5 * h / np.log(theta.shape[0]))\n",
    "\n",
    "#         # compute the rbf kernel\n",
    "#         Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "#         dxkxy = -np.matmul(Kxy, theta)\n",
    "#         sumkxy = np.sum(Kxy, axis=1)\n",
    "#         for i in range(theta.shape[1]):\n",
    "#             dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "#         dxkxy = dxkxy / (h**2)\n",
    "#         return (Kxy, dxkxy)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import json\n",
    "#     import sys\n",
    "#     import math\n",
    "    \n",
    "#     sess =  backend.get_session()\n",
    "\n",
    "#     attack = LinfBLOBAttack(m4,\n",
    "#                          epsilon=0.3,\n",
    "#                          k=200,\n",
    "#                          a=0.01,\n",
    "#                          random_start=True)\n",
    "\n",
    "#     num_eval_examples = 1000\n",
    "#     eval_batch_size = 200\n",
    "#     num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
    "\n",
    "#     print('Iterating over {} batches'.format(num_batches))\n",
    "\n",
    "#     #     x_adv_final = np.copy(mnist.test.images)\n",
    "\n",
    "#     for restart in range(50):\n",
    "#       # Initialize permutation\n",
    "#         permutation = np.arange(num_eval_examples)\n",
    "#         idx = np.arange(num_eval_examples)\n",
    "\n",
    "        \n",
    "#         (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#         Y_test = np.squeeze(Y_test)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "        \n",
    "#         x_test = X_test \n",
    "#         y_test = Y_test\n",
    "        \n",
    "#         x_adv = x_test + np.random.uniform(-attack.epsilon, attack.epsilon, x_test.shape)\n",
    "        \n",
    "\n",
    "#       # per round\n",
    "#         t0 = time.time()\n",
    "#         for epoch in range(int(attack.k/10)):\n",
    "#             print(epoch)\n",
    "#             np.random.shuffle(idx)\n",
    "#             x_test, x_adv, y_test = x_test[idx], x_adv[idx], y_test[idx]\n",
    "#             permutation = permutation[idx]\n",
    "            \n",
    "#             for ibatch in range(num_batches):\n",
    "#                 bstart = ibatch * eval_batch_size\n",
    "#                 bend = min(bstart + eval_batch_size, num_eval_examples)\n",
    "                \n",
    "#                 x_batch = x_test[bstart:bend, :]\n",
    "#                 x_batch_adv = x_adv[bstart:bend, :]\n",
    "#                 y_batch = y_test[bstart:bend]\n",
    "#             x_adv[bstart:bend, :] = attack.perturb(x_batch, x_batch_adv, y_batch, sess)\n",
    "            \n",
    "\n",
    "#         inv_permutation = np.argsort(permutation)\n",
    "#         x_adv = x_adv[inv_permutation]\n",
    " \n",
    "        \n",
    "#         t1 = time.time()\n",
    "\n",
    "#         print('restart: ', restart, '   time per batch: ', t1 - t0)\n",
    "\n",
    "#         print('L2: ', np.mean(np.square(x_adv - x_test)))\n",
    "#         print('Linf: ', np.max(np.abs(x_adv - x_test)))\n",
    "#         x_adv = tf.convert_to_tensor(x_adv)\n",
    "#         x_adv = tf.dtypes.cast(x_adv, tf.float32)\n",
    "#         preds = np.argmax(sess.run(attack.model.predict(x_adv)), 1)\n",
    "#         acc = np.mean(np.equal(preds, y_test))\n",
    "#         print('adv acc: ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver",
   "language": "python",
   "name": "adver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
