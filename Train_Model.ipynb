{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/henv python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This script trains a model that uses ECOC coding. It defines many types of models (baseline and ensemble). \n",
    "Uncomment the final two lines corresponding to the model of interest from one of the below model definition \"code blocks\" to train that model. \n",
    "Next run \"AttackModel\" to then attack this model.\n",
    "\"\"\"\n",
    "\n",
    "#IMPORTS \n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from Model_Implementations import Model_Softmax_Baseline, Model_Logistic_Baseline, Model_Logistic_Ensemble, Model_Tanh_Ensemble, Model_Tanh_Baseline\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL PARAMETERS - SET THESE APPROPRIATELY\n",
    "model_path = 'models/'  #path to save model weights to\n",
    "weight_save_freq = 1  #how frequently (in epochs, e.g. every 10 epochs) to save weights to disk\n",
    "tf.set_random_seed(1) \n",
    "\n",
    "\n",
    "########DATASET-SPECIFIC PARAMETERS: CHOOSE THIS BLOCK FOR MNIST\n",
    "#DATA_DESC = 'MNIST'; (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "#num_channels = 1; inp_shape = (28,28,1); num_classes=10\n",
    "##MODEL-SPECIFIC PARAMETERS: MNIST\n",
    "##PARAMETERS RELATED TO SGD OPTIMIZATION\n",
    "#epochs=150; batch_size=200; lr=3e-4; \n",
    "##MODEL DEFINTION PARAMETERS\n",
    "#num_filters_std = [64, 64, 64]; num_filters_ens=[32, 32, 32]; num_filters_ens_2=4; \n",
    "#dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "#noise_stddev = 0.3; blend_factor=0.3; \n",
    "#model_rep_baseline=1; model_rep_ens=2; \n",
    "#DATA_AUGMENTATION_FLAG=0; BATCH_NORMALIZATION_FLAG=0\n",
    "########END: DATASET-SPECIFIC PARAMETERS: MNIST\n",
    "\n",
    "\n",
    "##########DATASET-SPECIFIC PARAMETERS: CHOOSE THIS BLOCK FOR CIFAR10\n",
    "DATA_DESC = 'CIFAR10'; (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "Y_train = np.squeeze(Y_train); Y_test = np.squeeze(Y_test)\n",
    "num_channels = 3; inp_shape = (32,32,3); num_classes=10\n",
    "#MODEL-SPECIFIC PARAMETERS: CIFAR10\n",
    "#PARAMETERS RELATED TO SGD OPTIMIZATION\n",
    "epochs=300; batch_size=200; lr=2e-4; \n",
    "#MODEL DEFINTION PARAMETERS\n",
    "num_filters_std = [32, 64, 128]; num_filters_ens=[32, 64, 128]; num_filters_ens_2=16; \n",
    "dropout_rate_std=0.0; dropout_rate_ens=0.0; weight_decay = 0 \n",
    "noise_stddev = 0.032; blend_factor=0.032; \n",
    "model_rep_baseline=2; model_rep_ens=2; \n",
    "DATA_AUGMENTATION_FLAG=1; BATCH_NORMALIZATION_FLAG=1\n",
    "##########END: DATASET-SPECIFIC PARAMETERS: CIFAR10\n",
    "\n",
    "\n",
    "\n",
    "# DATA PRE-PROCESSING\n",
    "X_train = (X_train/255).astype(np.float32);  X_test = (X_test/255).astype(np.float32); #scale data to (0,1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],num_channels); X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],num_channels)\n",
    "X_valid = X_test[0:1000]; Y_valid = Y_test[0:1000]; #validation data (to monitor accuracy during training)\n",
    "X_train = X_train-0.5; X_test = X_test-0.5; X_valid = X_valid-0.5; #map to range (-0.5,0.5)\n",
    "data_dict = {'X_train':X_train, 'Y_train_cat':Y_train, 'X_test':X_test, 'Y_test_cat':Y_test}\n",
    "\n",
    "\n",
    "\n",
    "### TRAIN MODEL. each block below corresponds to one of the models in Table 1 of the paper. In order to train, \n",
    "#   uncomment the final two lines of the block of interest and then run this script\n",
    "\n",
    "\n",
    "### BASELINE SOFTMAX MODEL DEFINITION\n",
    "name = 'softmax_baseline'+'_'+DATA_DESC; num_chunks=1\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "output_activation = 'softmax'; base_model=None\n",
    "params_dict = {'weight_decay':weight_decay, 'num_filters_std':num_filters_std, 'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'model_rep':model_rep_baseline, 'base_model':base_model, 'num_chunks':num_chunks, 'output_activation':output_activation,  'batch_size':batch_size, 'epochs':epochs, 'lr':lr, 'dropout_rate':dropout_rate_std,  'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "# m0 = Model_Softmax_Baseline(data_dict, params_dict)\n",
    "# m0.defineModel(); m0.trainModel()\n",
    "\n",
    "\n",
    "## BASELINE LOGISTIC MODEL DEFINITION\n",
    "name = 'logistic_baseline'+'_'+DATA_DESC; num_chunks=1\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "output_activation = 'sigmoid'; base_model=None\n",
    "params_dict = {'weight_decay':weight_decay, 'num_filters_std':num_filters_std, 'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'model_rep':model_rep_baseline, 'base_model':base_model, 'num_chunks':num_chunks, 'output_activation':output_activation,  'batch_size':batch_size, 'epochs':epochs, 'lr':lr, 'dropout_rate':dropout_rate_std,  'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "#m1 = Model_Logistic_Baseline(data_dict, params_dict)\n",
    "#m1.defineModel(); m1.trainModel()\n",
    "\n",
    "\n",
    "## BASELINE TANH MODEL DEFINITION\n",
    "name = 'Tanh_baseline_16'+'_'+DATA_DESC; seed = 59; num_chunks=1; code_length=16; num_codes=num_classes; code_length_true=code_length\n",
    "M = scipy.linalg.hadamard(code_length).astype(np.float32)\n",
    "M[np.arange(0, num_codes,2), 0]= -1#replace first col, which for this Hadamard construction is always 1, hence not a useful bit\n",
    "np.random.seed(seed); np.random.shuffle(M)\n",
    "idx=np.random.permutation(code_length)\n",
    "M = M[0:num_codes, idx[0:code_length_true]]\n",
    "base_model=None\n",
    "def output_activation(x):\n",
    "    return tf.nn.tanh(x)\n",
    "params_dict = {'weight_decay':weight_decay, 'num_filters_std':num_filters_std, 'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'model_rep':model_rep_baseline, 'base_model':base_model, 'num_chunks':num_chunks, 'output_activation':output_activation,  'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_std,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "m2 = Model_Tanh_Baseline(data_dict, params_dict)\n",
    "m2.defineModel(); m2.trainModel()\n",
    "\n",
    "## ENSEMBLE LOGISTIC MODEL DEFINITION\n",
    "name = 'logistic_diverse'+'_'+DATA_DESC; num_chunks=2\n",
    "M = np.eye(num_classes).astype(np.float32)\n",
    "base_model=None\n",
    "def output_activation(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "params_dict = {'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'base_model':base_model, 'num_chunks':num_chunks, 'model_rep': model_rep_ens, 'output_activation':output_activation, 'num_filters_ens':num_filters_ens, 'num_filters_ens_2':num_filters_ens_2,'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_ens,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "#m3 = Model_Logistic_Ensemble(data_dict, params_dict)\n",
    "#m3.defineModel(); m3.trainModel()\n",
    "\n",
    "\n",
    "\n",
    "#COMMENTS FOR ALL TANH ENSEMBLE MODELS: \n",
    "#1. num_chunks refers to how many models comprise the ensemble (4 is used in the paper); code_length/num_chunks shoould be an integer\n",
    "#2. output_activation is the function to apply to the logits\n",
    "#   a. one can use anything which gives support to positive and negative values (since output code has +1/-1 elements); tanh or identity maps both work\n",
    "#   b. in order to alleviate potential concerns of gradient masking with tanh, one can use identity as well\n",
    "#3. M is the actual coding matrix (referred to in the paper as H).  Each row is a codeword\n",
    "#   note that any random shuffle of a Hadmard matrix's rows or columns is still orthogonal\n",
    "#4. There is nothing particularly special about the seed (which effectively determines the coding matrix). \n",
    "#   We tried several seeds from 0-60 and found that all give comparable model performance (e.g. benign and adversarial accuracy). \n",
    "\n",
    "## ENSEMBLE TANH 16 MODEL DEFINITION\n",
    "name = 'tanh_16_diverse'+'_'+DATA_DESC; seed = 59; code_length=16; num_codes=code_length; num_chunks=4; base_model=None; \n",
    "def output_activation(x):\n",
    "    return tf.nn.tanh(x)\n",
    "M = scipy.linalg.hadamard(code_length).astype(np.float32)\n",
    "M[np.arange(0, num_codes,2), 0]= -1#replace first col, which for scipy's Hadamard construction is always 1, hence not a useful classifier; this change still ensures all codewords have dot product <=0; since our decoder ignores negative correlations anyway, this has no net effect on probability estimation\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(M)\n",
    "idx=np.random.permutation(code_length)\n",
    "M = M[0:num_codes, idx[0:code_length]]\n",
    "params_dict = {'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'base_model':base_model, 'num_chunks':num_chunks, 'model_rep': model_rep_ens, 'output_activation':output_activation, 'num_filters_ens':num_filters_ens, 'num_filters_ens_2':num_filters_ens_2,'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_ens,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "#m4 = Model_Tanh_Ensemble(data_dict, params_dict)\n",
    "#m4.defineModel();   m4.trainModel()\n",
    "\n",
    "\n",
    "\n",
    "## ENSEMBLE TANH 32 MODEL DEFINITION\n",
    "name = 'tanh_32_diverse'+'_'+DATA_DESC; seed = 59; code_length=32; num_codes=code_length; num_chunks=4; base_model=None;\n",
    "def output_activation(x):\n",
    "    return tf.nn.tanh(x)\n",
    "M = scipy.linalg.hadamard(code_length).astype(np.float32)\n",
    "M[np.arange(0, num_codes,2), 0]= -1#replace first col, which for scipy's Hadamard construction is always 1, hence not a useful classifier; this change still ensures all codewords have dot product <=0; since our decoder ignores negative correlations anyway, this has no net effect on probability estimation\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(M)\n",
    "idx=np.random.permutation(code_length)\n",
    "M = M[0:num_codes, idx[0:code_length]]\n",
    "params_dict = {'BATCH_NORMALIZATION_FLAG':BATCH_NORMALIZATION_FLAG, 'DATA_AUGMENTATION_FLAG':DATA_AUGMENTATION_FLAG, 'M':M, 'base_model':base_model, 'num_chunks':num_chunks, 'model_rep': model_rep_ens, 'output_activation':output_activation, 'num_filters_ens':num_filters_ens, 'num_filters_ens_2':num_filters_ens_2,'batch_size':batch_size, 'epochs':epochs, 'dropout_rate':dropout_rate_ens,  'lr':lr, 'blend_factor':blend_factor, 'inp_shape':inp_shape, 'noise_stddev':noise_stddev, 'weight_save_freq':weight_save_freq, 'name':name, 'model_path':model_path}\n",
    "#m5 = Model_Tanh_Ensemble(data_dict, params_dict)\n",
    "#m5.defineModel();   m5.trainModel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver",
   "language": "python",
   "name": "adver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
